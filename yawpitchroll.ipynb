{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Team_Project-1\\validimage\\1_Color.png: 384x640 1 box, 936.5ms\n",
      "Speed: 0.0ms preprocess, 936.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"modelFolder/model_v7.pt\")\n",
    "results = model(source=\"validimage/1_Color.png\",device=\"cpu\", project = \"results\",name=\"test6d\", conf=0.8 )\n",
    "for result in results:\n",
    "    image_points = result.masks.xyn\n",
    "    print(len(image_points))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PointCloud with 1461 points.]\n"
     ]
    }
   ],
   "source": [
    "from crop_ply import *\n",
    "xyz_output= cropPly(image_points,\"validimage/1.ply\")\n",
    "print(xyz_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 box, 1 energdrink, 884.5ms\n",
      "Speed: 0.0ms preprocess, 884.5ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4651e+02, 3.4224e+02, 7.2615e+02, 5.0804e+02, 9.5698e-01, 1.0000e+00],\n",
      "        [1.1994e+03, 4.5363e+02, 1.2797e+03, 5.7467e+02, 9.0629e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9570, 0.9063])\n",
      "data: tensor([[5.4651e+02, 3.4224e+02, 7.2615e+02, 5.0804e+02, 9.5698e-01, 1.0000e+00],\n",
      "        [1.1994e+03, 4.5363e+02, 1.2797e+03, 5.7467e+02, 9.0629e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 636.3329,  425.1424,  179.6395,  165.8050],\n",
      "        [1239.5671,  514.1487,   80.3252,  121.0425]])\n",
      "xywhn: tensor([[0.4971, 0.5905, 0.1403, 0.2303],\n",
      "        [0.9684, 0.7141, 0.0628, 0.1681]])\n",
      "xyxy: tensor([[ 546.5132,  342.2399,  726.1527,  508.0449],\n",
      "        [1199.4045,  453.6274, 1279.7297,  574.6699]])\n",
      "xyxyn: tensor([[0.4270, 0.4753, 0.5673, 0.7056],\n",
      "        [0.9370, 0.6300, 0.9998, 0.7982]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 924.0ms\n",
      "Speed: 2.0ms preprocess, 924.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4682e+02, 3.3740e+02, 7.2734e+02, 5.0809e+02, 9.6141e-01, 1.0000e+00],\n",
      "        [1.1961e+03, 4.4766e+02, 1.2794e+03, 5.7464e+02, 9.0016e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9614, 0.9002])\n",
      "data: tensor([[5.4682e+02, 3.3740e+02, 7.2734e+02, 5.0809e+02, 9.6141e-01, 1.0000e+00],\n",
      "        [1.1961e+03, 4.4766e+02, 1.2794e+03, 5.7464e+02, 9.0016e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 637.0820,  422.7466,  180.5173,  170.6930],\n",
      "        [1237.7715,  511.1486,   83.2544,  126.9760]])\n",
      "xywhn: tensor([[0.4977, 0.5871, 0.1410, 0.2371],\n",
      "        [0.9670, 0.7099, 0.0650, 0.1764]])\n",
      "xyxy: tensor([[ 546.8234,  337.4001,  727.3407,  508.0931],\n",
      "        [1196.1443,  447.6606, 1279.3987,  574.6366]])\n",
      "xyxyn: tensor([[0.4272, 0.4686, 0.5682, 0.7057],\n",
      "        [0.9345, 0.6218, 0.9995, 0.7981]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 1061.6ms\n",
      "Speed: 0.0ms preprocess, 1061.6ms inference, 10.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4676e+02, 3.3754e+02, 7.2735e+02, 5.0854e+02, 9.6128e-01, 1.0000e+00],\n",
      "        [1.1965e+03, 4.4801e+02, 1.2794e+03, 5.7429e+02, 9.0341e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9613, 0.9034])\n",
      "data: tensor([[5.4676e+02, 3.3754e+02, 7.2735e+02, 5.0854e+02, 9.6128e-01, 1.0000e+00],\n",
      "        [1.1965e+03, 4.4801e+02, 1.2794e+03, 5.7429e+02, 9.0341e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 637.0546,  423.0408,  180.5835,  171.0012],\n",
      "        [1237.9736,  511.1475,   82.9307,  126.2784]])\n",
      "xywhn: tensor([[0.4977, 0.5876, 0.1411, 0.2375],\n",
      "        [0.9672, 0.7099, 0.0648, 0.1754]])\n",
      "xyxy: tensor([[ 546.7628,  337.5402,  727.3463,  508.5414],\n",
      "        [1196.5083,  448.0083, 1279.4390,  574.2867]])\n",
      "xyxyn: tensor([[0.4272, 0.4688, 0.5682, 0.7063],\n",
      "        [0.9348, 0.6222, 0.9996, 0.7976]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 1023.6ms\n",
      "Speed: 4.4ms preprocess, 1023.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4682e+02, 3.3770e+02, 7.2725e+02, 5.0852e+02, 9.6357e-01, 1.0000e+00],\n",
      "        [1.1965e+03, 4.4702e+02, 1.2793e+03, 5.7382e+02, 8.5772e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9636, 0.8577])\n",
      "data: tensor([[5.4682e+02, 3.3770e+02, 7.2725e+02, 5.0852e+02, 9.6357e-01, 1.0000e+00],\n",
      "        [1.1965e+03, 4.4702e+02, 1.2793e+03, 5.7382e+02, 8.5772e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 637.0331,  423.1125,  180.4296,  170.8169],\n",
      "        [1237.8972,  510.4203,   82.8779,  126.7920]])\n",
      "xywhn: tensor([[0.4977, 0.5877, 0.1410, 0.2372],\n",
      "        [0.9671, 0.7089, 0.0647, 0.1761]])\n",
      "xyxy: tensor([[ 546.8183,  337.7041,  727.2479,  508.5210],\n",
      "        [1196.4583,  447.0243, 1279.3362,  573.8163]])\n",
      "xyxyn: tensor([[0.4272, 0.4690, 0.5682, 0.7063],\n",
      "        [0.9347, 0.6209, 0.9995, 0.7970]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 986.5ms\n",
      "Speed: 2.0ms preprocess, 986.5ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4691e+02, 3.3747e+02, 7.2741e+02, 5.0882e+02, 9.6065e-01, 1.0000e+00],\n",
      "        [1.1965e+03, 4.4828e+02, 1.2794e+03, 5.7505e+02, 8.9668e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9606, 0.8967])\n",
      "data: tensor([[5.4691e+02, 3.3747e+02, 7.2741e+02, 5.0882e+02, 9.6065e-01, 1.0000e+00],\n",
      "        [1.1965e+03, 4.4828e+02, 1.2794e+03, 5.7505e+02, 8.9668e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 637.1587,  423.1486,  180.5055,  171.3478],\n",
      "        [1237.9363,  511.6659,   82.8857,  126.7683]])\n",
      "xywhn: tensor([[0.4978, 0.5877, 0.1410, 0.2380],\n",
      "        [0.9671, 0.7106, 0.0648, 0.1761]])\n",
      "xyxy: tensor([[ 546.9059,  337.4747,  727.4114,  508.8225],\n",
      "        [1196.4934,  448.2817, 1279.3792,  575.0500]])\n",
      "xyxyn: tensor([[0.4273, 0.4687, 0.5683, 0.7067],\n",
      "        [0.9348, 0.6226, 0.9995, 0.7987]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 983.7ms\n",
      "Speed: 2.8ms preprocess, 983.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4686e+02, 3.3774e+02, 7.2734e+02, 5.0844e+02, 9.5989e-01, 1.0000e+00],\n",
      "        [1.1958e+03, 4.4847e+02, 1.2794e+03, 5.7485e+02, 8.9833e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9599, 0.8983])\n",
      "data: tensor([[5.4686e+02, 3.3774e+02, 7.2734e+02, 5.0844e+02, 9.5989e-01, 1.0000e+00],\n",
      "        [1.1958e+03, 4.4847e+02, 1.2794e+03, 5.7485e+02, 8.9833e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 637.1003,  423.0911,  180.4854,  170.7004],\n",
      "        [1237.5946,  511.6599,   83.6455,  126.3865]])\n",
      "xywhn: tensor([[0.4977, 0.5876, 0.1410, 0.2371],\n",
      "        [0.9669, 0.7106, 0.0653, 0.1755]])\n",
      "xyxy: tensor([[ 546.8576,  337.7408,  727.3430,  508.4413],\n",
      "        [1195.7719,  448.4667, 1279.4174,  574.8531]])\n",
      "xyxyn: tensor([[0.4272, 0.4691, 0.5682, 0.7062],\n",
      "        [0.9342, 0.6229, 0.9995, 0.7984]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 973.3ms\n",
      "Speed: 0.0ms preprocess, 973.3ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4675e+02, 3.3744e+02, 7.2690e+02, 5.0856e+02, 9.6437e-01, 1.0000e+00],\n",
      "        [1.1960e+03, 4.4821e+02, 1.2794e+03, 5.7481e+02, 8.9571e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9644, 0.8957])\n",
      "data: tensor([[5.4675e+02, 3.3744e+02, 7.2690e+02, 5.0856e+02, 9.6437e-01, 1.0000e+00],\n",
      "        [1.1960e+03, 4.4821e+02, 1.2794e+03, 5.7481e+02, 8.9571e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 636.8285,  422.9968,  180.1492,  171.1174],\n",
      "        [1237.7334,  511.5110,   83.3840,  126.5952]])\n",
      "xywhn: tensor([[0.4975, 0.5875, 0.1407, 0.2377],\n",
      "        [0.9670, 0.7104, 0.0651, 0.1758]])\n",
      "xyxy: tensor([[ 546.7539,  337.4381,  726.9031,  508.5555],\n",
      "        [1196.0414,  448.2134, 1279.4254,  574.8086]])\n",
      "xyxyn: tensor([[0.4272, 0.4687, 0.5679, 0.7063],\n",
      "        [0.9344, 0.6225, 0.9996, 0.7983]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 963.0ms\n",
      "Speed: 0.0ms preprocess, 963.0ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4665e+02, 3.3738e+02, 7.2735e+02, 5.0853e+02, 9.6272e-01, 1.0000e+00],\n",
      "        [1.1960e+03, 4.4850e+02, 1.2794e+03, 5.7531e+02, 8.8306e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9627, 0.8831])\n",
      "data: tensor([[5.4665e+02, 3.3738e+02, 7.2735e+02, 5.0853e+02, 9.6272e-01, 1.0000e+00],\n",
      "        [1.1960e+03, 4.4850e+02, 1.2794e+03, 5.7531e+02, 8.8306e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 637.0045,  422.9572,  180.6996,  171.1531],\n",
      "        [1237.6846,  511.9083,   83.3721,  126.8132]])\n",
      "xywhn: tensor([[0.4977, 0.5874, 0.1412, 0.2377],\n",
      "        [0.9669, 0.7110, 0.0651, 0.1761]])\n",
      "xyxy: tensor([[ 546.6547,  337.3807,  727.3542,  508.5338],\n",
      "        [1195.9985,  448.5017, 1279.3706,  575.3149]])\n",
      "xyxyn: tensor([[0.4271, 0.4686, 0.5682, 0.7063],\n",
      "        [0.9344, 0.6229, 0.9995, 0.7990]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 977.2ms\n",
      "Speed: 0.0ms preprocess, 977.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4703e+02, 3.3788e+02, 7.2742e+02, 5.0870e+02, 9.6086e-01, 1.0000e+00],\n",
      "        [1.1973e+03, 4.5002e+02, 1.2794e+03, 5.7488e+02, 9.0011e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9609, 0.9001])\n",
      "data: tensor([[5.4703e+02, 3.3788e+02, 7.2742e+02, 5.0870e+02, 9.6086e-01, 1.0000e+00],\n",
      "        [1.1973e+03, 4.5002e+02, 1.2794e+03, 5.7488e+02, 9.0011e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 637.2248,  423.2874,  180.3962,  170.8206],\n",
      "        [1238.3694,  512.4465,   82.1187,  124.8582]])\n",
      "xywhn: tensor([[0.4978, 0.5879, 0.1409, 0.2373],\n",
      "        [0.9675, 0.7117, 0.0642, 0.1734]])\n",
      "xyxy: tensor([[ 547.0267,  337.8771,  727.4229,  508.6977],\n",
      "        [1197.3101,  450.0174, 1279.4287,  574.8756]])\n",
      "xyxyn: tensor([[0.4274, 0.4693, 0.5683, 0.7065],\n",
      "        [0.9354, 0.6250, 0.9996, 0.7984]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 934.4ms\n",
      "Speed: 8.0ms preprocess, 934.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4688e+02, 3.3759e+02, 7.2688e+02, 5.0813e+02, 9.6300e-01, 1.0000e+00],\n",
      "        [1.1963e+03, 4.4962e+02, 1.2794e+03, 5.7493e+02, 8.9613e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9630, 0.8961])\n",
      "data: tensor([[5.4688e+02, 3.3759e+02, 7.2688e+02, 5.0813e+02, 9.6300e-01, 1.0000e+00],\n",
      "        [1.1963e+03, 4.4962e+02, 1.2794e+03, 5.7493e+02, 8.9613e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 636.8787,  422.8580,  179.9990,  170.5374],\n",
      "        [1237.8687,  512.2759,   83.0986,  125.3105]])\n",
      "xywhn: tensor([[0.4976, 0.5873, 0.1406, 0.2369],\n",
      "        [0.9671, 0.7115, 0.0649, 0.1740]])\n",
      "xyxy: tensor([[ 546.8792,  337.5893,  726.8782,  508.1267],\n",
      "        [1196.3193,  449.6206, 1279.4180,  574.9312]])\n",
      "xyxyn: tensor([[0.4272, 0.4689, 0.5679, 0.7057],\n",
      "        [0.9346, 0.6245, 0.9995, 0.7985]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 1023.4ms\n",
      "Speed: 10.1ms preprocess, 1023.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4696e+02, 3.3737e+02, 7.2732e+02, 5.0855e+02, 9.6025e-01, 1.0000e+00],\n",
      "        [1.1967e+03, 4.4838e+02, 1.2794e+03, 5.7439e+02, 9.0016e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9602, 0.9002])\n",
      "data: tensor([[5.4696e+02, 3.3737e+02, 7.2732e+02, 5.0855e+02, 9.6025e-01, 1.0000e+00],\n",
      "        [1.1967e+03, 4.4838e+02, 1.2794e+03, 5.7439e+02, 9.0016e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 637.1382,  422.9620,  180.3580,  171.1815],\n",
      "        [1238.0516,  511.3817,   82.7532,  126.0094]])\n",
      "xywhn: tensor([[0.4978, 0.5874, 0.1409, 0.2378],\n",
      "        [0.9672, 0.7103, 0.0647, 0.1750]])\n",
      "xyxy: tensor([[ 546.9592,  337.3713,  727.3173,  508.5528],\n",
      "        [1196.6750,  448.3770, 1279.4282,  574.3864]])\n",
      "xyxyn: tensor([[0.4273, 0.4686, 0.5682, 0.7063],\n",
      "        [0.9349, 0.6227, 0.9996, 0.7978]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 990.6ms\n",
      "Speed: 0.0ms preprocess, 990.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4688e+02, 3.3744e+02, 7.2695e+02, 5.0850e+02, 9.6296e-01, 1.0000e+00],\n",
      "        [1.1963e+03, 4.4808e+02, 1.2794e+03, 5.7429e+02, 8.9690e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9630, 0.8969])\n",
      "data: tensor([[5.4688e+02, 3.3744e+02, 7.2695e+02, 5.0850e+02, 9.6296e-01, 1.0000e+00],\n",
      "        [1.1963e+03, 4.4808e+02, 1.2794e+03, 5.7429e+02, 8.9690e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 636.9158,  422.9684,  180.0770,  171.0626],\n",
      "        [1237.8529,  511.1823,   83.0444,  126.2092]])\n",
      "xywhn: tensor([[0.4976, 0.5875, 0.1407, 0.2376],\n",
      "        [0.9671, 0.7100, 0.0649, 0.1753]])\n",
      "xyxy: tensor([[ 546.8773,  337.4371,  726.9543,  508.4998],\n",
      "        [1196.3307,  448.0777, 1279.3751,  574.2869]])\n",
      "xyxyn: tensor([[0.4272, 0.4687, 0.5679, 0.7062],\n",
      "        [0.9346, 0.6223, 0.9995, 0.7976]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 903.5ms\n",
      "Speed: 0.0ms preprocess, 903.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4680e+02, 3.3696e+02, 7.2711e+02, 5.0836e+02, 9.6312e-01, 1.0000e+00],\n",
      "        [1.1968e+03, 4.4837e+02, 1.2794e+03, 5.7473e+02, 8.9209e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9631, 0.8921])\n",
      "data: tensor([[5.4680e+02, 3.3696e+02, 7.2711e+02, 5.0836e+02, 9.6312e-01, 1.0000e+00],\n",
      "        [1.1968e+03, 4.4837e+02, 1.2794e+03, 5.7473e+02, 8.9209e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 636.9545,  422.6620,  180.3184,  171.3983],\n",
      "        [1238.1252,  511.5492,   82.5781,  126.3576]])\n",
      "xywhn: tensor([[0.4976, 0.5870, 0.1409, 0.2381],\n",
      "        [0.9673, 0.7105, 0.0645, 0.1755]])\n",
      "xyxy: tensor([[ 546.7953,  336.9629,  727.1137,  508.3612],\n",
      "        [1196.8362,  448.3704, 1279.4143,  574.7280]])\n",
      "xyxyn: tensor([[0.4272, 0.4680, 0.5681, 0.7061],\n",
      "        [0.9350, 0.6227, 0.9995, 0.7982]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 1019.7ms\n",
      "Speed: 8.1ms preprocess, 1019.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4680e+02, 3.3729e+02, 7.2708e+02, 5.0848e+02, 9.6252e-01, 1.0000e+00],\n",
      "        [1.1967e+03, 4.4902e+02, 1.2794e+03, 5.7510e+02, 8.9772e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9625, 0.8977])\n",
      "data: tensor([[5.4680e+02, 3.3729e+02, 7.2708e+02, 5.0848e+02, 9.6252e-01, 1.0000e+00],\n",
      "        [1.1967e+03, 4.4902e+02, 1.2794e+03, 5.7510e+02, 8.9772e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 636.9406,  422.8857,  180.2816,  171.1841],\n",
      "        [1238.0879,  512.0579,   82.7080,  126.0821]])\n",
      "xywhn: tensor([[0.4976, 0.5873, 0.1408, 0.2378],\n",
      "        [0.9673, 0.7112, 0.0646, 0.1751]])\n",
      "xyxy: tensor([[ 546.7998,  337.2937,  727.0814,  508.4778],\n",
      "        [1196.7339,  449.0168, 1279.4419,  575.0989]])\n",
      "xyxyn: tensor([[0.4272, 0.4685, 0.5680, 0.7062],\n",
      "        [0.9349, 0.6236, 0.9996, 0.7987]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 871.8ms\n",
      "Speed: 10.1ms preprocess, 871.8ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4683e+02, 3.3726e+02, 7.2702e+02, 5.0843e+02, 9.6372e-01, 1.0000e+00],\n",
      "        [1.1963e+03, 4.4809e+02, 1.2794e+03, 5.7467e+02, 8.9796e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9637, 0.8980])\n",
      "data: tensor([[5.4683e+02, 3.3726e+02, 7.2702e+02, 5.0843e+02, 9.6372e-01, 1.0000e+00],\n",
      "        [1.1963e+03, 4.4809e+02, 1.2794e+03, 5.7467e+02, 8.9796e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 636.9258,  422.8446,  180.1917,  171.1666],\n",
      "        [1237.8408,  511.3798,   83.1375,  126.5852]])\n",
      "xywhn: tensor([[0.4976, 0.5873, 0.1408, 0.2377],\n",
      "        [0.9671, 0.7102, 0.0650, 0.1758]])\n",
      "xyxy: tensor([[ 546.8300,  337.2614,  727.0216,  508.4279],\n",
      "        [1196.2721,  448.0872, 1279.4095,  574.6724]])\n",
      "xyxyn: tensor([[0.4272, 0.4684, 0.5680, 0.7061],\n",
      "        [0.9346, 0.6223, 0.9995, 0.7982]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 box, 1 energdrink, 1048.3ms\n",
      "Speed: 2.0ms preprocess, 1048.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[5.4678e+02, 3.3708e+02, 7.2647e+02, 5.0855e+02, 9.6438e-01, 1.0000e+00],\n",
      "        [1.1965e+03, 4.4832e+02, 1.2794e+03, 5.7501e+02, 8.9226e-01, 3.0000e+00]])\n",
      "cls: tensor([1., 3.])\n",
      "conf: tensor([0.9644, 0.8923])\n",
      "data: tensor([[5.4678e+02, 3.3708e+02, 7.2647e+02, 5.0855e+02, 9.6438e-01, 1.0000e+00],\n",
      "        [1.1965e+03, 4.4832e+02, 1.2794e+03, 5.7501e+02, 8.9226e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[ 636.6234,  422.8131,  179.6903,  171.4654],\n",
      "        [1237.9589,  511.6634,   82.9277,  126.6904]])\n",
      "xywhn: tensor([[0.4974, 0.5872, 0.1404, 0.2381],\n",
      "        [0.9672, 0.7106, 0.0648, 0.1760]])\n",
      "xyxy: tensor([[ 546.7782,  337.0804,  726.4685,  508.5458],\n",
      "        [1196.4950,  448.3182, 1279.4227,  575.0086]])\n",
      "xyxyn: tensor([[0.4272, 0.4682, 0.5676, 0.7063],\n",
      "        [0.9348, 0.6227, 0.9995, 0.7986]])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# RealSense 카메라 스트리밍 설정\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640,360, rs.format.z16,30)\n",
    "pipeline.start(config)\n",
    "\n",
    "# YOLO 모델 로드 (예: Darknet 또는 다른 라이브러리 사용)\n",
    "model = YOLO(\"modelFolder/model_v7.pt\")\n",
    "\n",
    "# 클래스별 고유한 색상 정의\n",
    "class_colors = {\n",
    "     0: (0, 255, 0),   # 클래스 0의 색상 (예: 녹색)\n",
    "    1: (255, 0, 0),   # 클래스 1의 색상 (예: 빨강)\n",
    "    2: (0, 0, 255),   # 클래스 2의 색상 (예: 파랑)\n",
    "    3: (255, 255, 0), # 클래스 3의 색상 (예: 노랑)\n",
    "    4: (128, 0, 128), # 클래스 4의 색상 (예: 보라)\n",
    "    \n",
    "}\n",
    "\n",
    "while True:\n",
    "    # RealSense 카메라 프레임 가져오기\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    if not color_frame:\n",
    "        continue\n",
    "\n",
    "    # RealSense 프레임을 OpenCV 이미지로 변환\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    # YOLO를 사용하여 객체 검출 및 세그멘테이션 수행\n",
    "    results = model(color_image, conf=0.8,device=\"cpu\")\n",
    "   # 결과를 color_image에 오버레이\n",
    "    for result in results:\n",
    "        print(result.boxes)\n",
    "       \n",
    "        for box,cls,conf in zip(result.boxes.xywh.tolist(), result.boxes.cls.tolist(),result.boxes.conf.tolist()):\n",
    "            # print(\"r\",box)\n",
    "            # print(\"cls\",cls)\n",
    "            x, y, w, h = box\n",
    "            x1, y1, x2, y2 = int(x - w / 2), int(y - h / 2), int(x + w / 2), int(y + h / 2)\n",
    "            label = f\"{model.names[int(cls)]}: {conf:.2f}\"\n",
    "            \n",
    "            color_image = cv2.rectangle(color_image, (x1, y1), (x2, y2), class_colors[cls], 2)\n",
    "            color_image = cv2.putText(color_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "\n",
    "    # 결과를 color_image에 오버레이 또는 다른 방식으로 표시\n",
    "\n",
    "    # 화면에 결과 표시\n",
    "    cv2.imshow(\"YOLO Segmentation\", color_image)\n",
    "\n",
    "    # 'q' 키를 눌러 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 정리 및 종료\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -0.040005    -0.02612    -0.64369]\n",
      "(x 축 기준): 177.53971780029164도\n",
      "(y 축 기준): 90.01150427849596도\n",
      "(z 축 기준): 92.46025526947014도\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# 포인트 클라우드를 시각화\n",
    "for cloud in xyz_output:\n",
    "    # 포인트 클라우드의 경계 상자 계산\n",
    "    aabb = cloud.get_axis_aligned_bounding_box()\n",
    "    aabb.color = (1, 0, 0)\n",
    "\n",
    "    obb = cloud.get_oriented_bounding_box()\n",
    "    obb.color = (0, 1, 0)\n",
    "\n",
    "    # 클라우드 포인트의 중심 위치 계산\n",
    "    center = np.mean(np.asarray(cloud.points), axis=0)\n",
    "    print(center)\n",
    "\n",
    "    # 중앙 위치에 좌표 프레임 메시 생성\n",
    "    mesh = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.07)  # size를 조절하여 좌표 프레임 크기 설정 가능\n",
    "\n",
    "    # 중앙 위치에 좌표 프레임 메시를 이동\n",
    "    mesh.translate(center)\n",
    "\n",
    "    # 시각화\n",
    "    o3d.visualization.draw_geometries([cloud, aabb, obb, mesh])\n",
    "\n",
    "    # PCA를 수행하여 주요 주성분 벡터를 찾습니다.\n",
    "    points = np.asarray(cloud.points)\n",
    "    covariance_matrix = np.cov(points, rowvar=False)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "    # 주요 주성분 벡터와 각각의 축과의 각도를 계산합니다.\n",
    "    x_axis = np.array([1, 0, 0])  # x 축\n",
    "    y_axis = np.array([0, 1, 0])  # y 축\n",
    "    z_axis = np.array([0, 0, 1])  # z 축\n",
    "\n",
    "    angle_x_rad = np.arccos(np.dot(eigenvectors[:, -1], x_axis) / (np.linalg.norm(eigenvectors[:, -1]) * np.linalg.norm(x_axis)))\n",
    "    angle_x_deg = np.degrees(angle_x_rad)\n",
    "\n",
    "    angle_y_rad = np.arccos(np.dot(eigenvectors[:, -1], y_axis) / (np.linalg.norm(eigenvectors[:, -1]) * np.linalg.norm(y_axis)))\n",
    "    angle_y_deg = np.degrees(angle_y_rad)\n",
    "\n",
    "    angle_z_rad = np.arccos(np.dot(eigenvectors[:, -1], z_axis) / (np.linalg.norm(eigenvectors[:, -1]) * np.linalg.norm(z_axis)))\n",
    "    angle_z_deg = np.degrees(angle_z_rad)\n",
    "\n",
    "    \n",
    "    print(f\"(x 축 기준): {angle_x_deg}도\")\n",
    "    print(f\"(y 축 기준): {angle_y_deg}도\")\n",
    "    print(f\"(z 축 기준): {angle_z_deg}도\")\n",
    "    \n",
    "    o3d.io.write_point_cloud(\"robot/test.ply\", cloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xyz_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Team_Project-1\\yawpitchroll.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Team_Project-1/yawpitchroll.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Team_Project-1/yawpitchroll.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 포인트 클라우드를 시각화\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Team_Project-1/yawpitchroll.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m cloud \u001b[39min\u001b[39;00m xyz_output:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Team_Project-1/yawpitchroll.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# 포인트 클라우드의 경계 상자 계산\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Team_Project-1/yawpitchroll.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     aabb \u001b[39m=\u001b[39m cloud\u001b[39m.\u001b[39mget_axis_aligned_bounding_box()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Team_Project-1/yawpitchroll.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     aabb\u001b[39m.\u001b[39mcolor \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xyz_output' is not defined"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# 포인트 클라우드를 시각화\n",
    "for cloud in xyz_output:\n",
    "    # 포인트 클라우드의 경계 상자 계산\n",
    "    aabb = cloud.get_axis_aligned_bounding_box()\n",
    "    aabb.color = (1, 0, 0)\n",
    "\n",
    "    obb = cloud.get_oriented_bounding_box()\n",
    "    obb.color = (0, 1, 0)\n",
    "\n",
    "    # 클라우드 포인트의 중심 위치 계산\n",
    "    center = np.mean(np.asarray(cloud.points), axis=0)\n",
    "    print(center)\n",
    "\n",
    "    # 중앙 위치에 좌표 프레임 메시 생성\n",
    "    mesh = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.07)  # size를 조절하여 좌표 프레임 크기 설정 가능\n",
    "\n",
    "    # 중앙 위치에 좌표 프레임 메시를 이동\n",
    "    mesh.translate(center)\n",
    "\n",
    "    # 시각화\n",
    "    o3d.visualization.draw_geometries([cloud, aabb, obb, mesh])\n",
    "\n",
    "    # PCA를 수행하여 주요 주성분 벡터를 찾습니다.\n",
    "    points = np.asarray(cloud.points)\n",
    "    covariance_matrix = np.cov(points, rowvar=False)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "     # 주요 주성분 벡터와 중심 위치를 시각화\n",
    "    lines = []\n",
    "    for i in range(3):  # 주요 주성분 벡터의 개수는 3개입니다.\n",
    "        eigenvector = eigenvectors[:, i]  # i번째 주요 주성분 벡터 가져오기\n",
    "\n",
    "        # 중심 위치에서 주요 주성분 벡터를 시작하도록 끝점 좌표 계산\n",
    "        endpoint = center + eigenvector * 0.1  # 벡터의 크기를 조절하여 시각화 크기 설정 가능\n",
    "\n",
    "        # 주요 주성분 벡터를 라인 메시로 생성하여 시각화\n",
    "        line = o3d.geometry.LineSet()\n",
    "        line.points = o3d.utility.Vector3dVector(np.array([center, endpoint]))\n",
    "        line.lines = o3d.utility.Vector2iVector(np.array([[0, 1]]))\n",
    "\n",
    "        # 주요 주성분 벡터에 따라 다른 색상 설정\n",
    "        if i == 0:\n",
    "            line.colors = o3d.utility.Vector3dVector(np.array([[1, 0, 0]]))  # 빨간색\n",
    "        elif i == 1:\n",
    "            line.colors = o3d.utility.Vector3dVector(np.array([[0, 1, 0]]))  # 녹색\n",
    "        elif i == 2:\n",
    "            line.colors = o3d.utility.Vector3dVector(np.array([[0, 0, 1]]))  # 파란색\n",
    "        \n",
    "        lines.append(line)\n",
    "    \n",
    "\n",
    "      # 시각화에 추가\n",
    "    o3d.visualization.draw_geometries([cloud, aabb, obb, mesh, *lines])\n",
    "\n",
    "    # 포인트 클라우드를 PLY 파일로 저장\n",
    "    o3d.io.write_point_cloud(\"robot/test.ply\", cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카메라 메트릭스:\n",
      "Focal Length: (614.6367797851562, 614.6595458984375)\n",
      "Principal Point: (313.7270812988281, 246.65493774414062)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Team_Project-1\\yawpitchroll.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Team_Project-1/yawpitchroll.ipynb#X12sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Team_Project-1/yawpitchroll.ipynb#X12sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# 체스보드 코너 탐지\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Team_Project-1/yawpitchroll.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m ret, corners \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mfindChessboardCorners(gray, (board_width, board_height), \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Team_Project-1/yawpitchroll.ipynb#X12sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mif\u001b[39;00m ret:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Team_Project-1/yawpitchroll.ipynb#X12sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     objpoints\u001b[39m.\u001b[39mappend(objp)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyrealsense2 as rs\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# RealSense 카메라 객체 생성\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "# 카메라 시작\n",
    "pipeline.start(config)\n",
    "\n",
    "# RealSense 카메라 메트릭스 및 왜곡 계수 얻기\n",
    "frames = pipeline.wait_for_frames()\n",
    "color_frame = frames.get_color_frame()\n",
    "intrinsics = color_frame.profile.as_video_stream_profile().intrinsics\n",
    "\n",
    "# 카메라 메트릭스 출력\n",
    "print(\"카메라 메트릭스:\")\n",
    "print(f\"Focal Length: ({intrinsics.fx}, {intrinsics.fy})\")\n",
    "print(f\"Principal Point: ({intrinsics.ppx}, {intrinsics.ppy})\")\n",
    "\n",
    "# 체스보드의 가로 및 세로 내부 코너 개수\n",
    "board_width = 9\n",
    "board_height = 6\n",
    "\n",
    "# 체스보드 코너 좌표 초기화\n",
    "objp = np.zeros((board_width * board_height, 3), np.float32)\n",
    "objp[:, :2] = np.mgrid[0:board_width, 0:board_height].T.reshape(-1, 2)\n",
    "\n",
    "# 3D 포인트 배열과 2D 포인트 배열 초기화\n",
    "objpoints = []  # 3D 포인트 (실제 3D 체스보드 코너)\n",
    "imgpoints = []  # 2D 포인트 (이미지 상의 체스보드 코너)\n",
    "\n",
    "# VideoCapture 객체 생성 (카메라에 연결)\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "image_counter = 0  # 이미지 카운터 초기화\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # 프레임 캡처\n",
    "\n",
    "    if not ret:\n",
    "        continue  # 프레임을 읽지 못하면 다음 반복으로 넘어감\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 체스보드 코너 탐지\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (board_width, board_height), None)\n",
    "    \n",
    "    if ret:\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "        # 체스보드 코너 표시\n",
    "        cv2.drawChessboardCorners(frame, (board_width, board_height), corners, ret)\n",
    "        \n",
    "        # 이미지 저장\n",
    "        image_name = f\"calibration_image_{image_counter}.jpg\"\n",
    "        cv2.imwrite(image_name, frame)\n",
    "        image_counter += 1\n",
    "        # 'q' 키를 누르면 루프 종료\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 이미지 표시\n",
    "    cv2.imshow(\"Chessboard\", frame)\n",
    "\n",
    "# 캘리브레이션 수행 (이미지 저장 후에 수행)\n",
    "if len(objpoints) > 0 and len(imgpoints) > 0:\n",
    "    ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"카메라 메트릭스:\")\n",
    "    print(camera_matrix)\n",
    "    print(\"\\n왜곡 계수:\")\n",
    "    print(dist_coeffs)\n",
    "else:\n",
    "    print(\"체스보드를 감지하는 데 실패했습니다. 이미지를 확보하거나 체스보드를 올바르게 배치하세요.\")\n",
    "\n",
    "# 카메라 캡처 해제 및 창 닫기\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
